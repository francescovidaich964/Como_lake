{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-POIS Optimization test\n",
    "In this notebook we will build an optimization procedure for an agent that has the goal of keeping the Como Lake at a constant level. The inflow values that will be used start in 1946 and ends in 2011. More recent values can be found at this [link](https://adda.laghi.net/homepage.aspx?tab=3&subtab=2&idlago=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como Lake RL Environment\n",
    "We start by building the environment of the lake, which at each timestep will be represented by its water level (which will be the state $s_t$ and by the inflow $i_t$ of that day. In particular, the agent will choose its action $a_t$ knowing the current level of the lake but not the inflow that will be measured during that day. The reward $r_t$ will simply be equal the difference between the current level and the zero-level $s_0=197.37m$ of the Comolake.\n",
    "\n",
    "The update of the state is given by the following equation:\n",
    "\n",
    "$$ s_{t+1} = s_t + \\frac{(~\\text{inflow}_t - \\text{outflow}_t~)~n_{\\text{sec}}}{\\text{Area}} $$\n",
    "\n",
    "where $s$ represents the daily level of the lake (measured in $m$), $\\text{inflow}$ and $\\text{outflow}$ values are averaged over their corresponding day (measured in $m^3/s$), $n_\\text{sec}$ is the number of seconds in a day and $\\text{Area}$ represents the surface area of the lake. For the moment I found $\\text{Area}=146km^2$, but a more accurate value should be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class como_lake:\n",
    "    \n",
    "    def __init__(self, s0=197.37, t0=0, inflows=None):\n",
    "        \n",
    "        # Define initial and objective levels of water\n",
    "        self.state = s0\n",
    "        self.zero_state = 197.37\n",
    "        \n",
    "        # If there are not custom inflow values, take historic data\n",
    "        if inflows == None:\n",
    "            self.inflows = np.genfromtxt('data/como_data_1946_2011.txt', delimiter=\"  \")[1:,5]\n",
    "        self.t_limit = len(self.inflows)\n",
    "        self.time_id = t0     \n",
    "        \n",
    "        \n",
    "    def step(self, outflow, inflow=None):\n",
    "        \n",
    "        ### Function returns:  new_state, reward, end_flag \n",
    "        \n",
    "        # Take inflow value from data if not provided\n",
    "        if inflow == None:\n",
    "            inflow = inflows[self.time_id]\n",
    "        \n",
    "        # Check if:  - Outflow is positive -> if negative consider it equal to 0\n",
    "        #            - There is enough water to release -> if not, clip it to the current level\n",
    "        outflow = np.clip(outflow, 0, self.state*146000000/86400)\n",
    "        \n",
    "        # Compute new level of the lake after action and inflow\n",
    "        in_out_diff = inflows[self.time_id] - outflow\n",
    "        self.state += in_out_diff*86400 / 146000000\n",
    "        \n",
    "        # Check if there still is inflow data: if yes, update time index\n",
    "        if self.time_id >= self.t_limit:\n",
    "            return self.state, -np.abs(self.state-self.zero_state), True\n",
    "        else:\n",
    "            self.time_id += 1\n",
    "            return self.state, -np.abs(self.state-self.zero_state), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Policy and Policies\n",
    "The policy is linear and deterministic, thus we have $a = \\pi_{\\theta_t}(s) = W_t s + b_t$ and $\\theta_t=\\{W_t,b_t\\}$, but we will clip it to $0$ if it tries to release a negative quantity of water. Then, the hyper-policy will be $\\nu_\\rho(W_t,b_t|t) = \\left( \\mathcal{N}(\\mu_{W,t},\\Sigma_{W,t}), \\mathcal{N}(\\mu_{b,t},\\Sigma_{b,t}) \\right)$, with:\n",
    " - $\\mu_{W,t} = \\nu_{\\rho,W}(t) = A_{W} \\sin(\\phi_{W} t + \\psi_{W}) + B_{W}$\n",
    " - $\\mu_{b,t} = \\nu_{\\rho,b}(t) = A_{b} \\sin(\\phi_{b} t + \\psi_{b}) + B_{b}$\n",
    " - $\\Sigma_{W,t}$ and $\\Sigma_{b,t}$ given as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_policy:\n",
    "    \n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        return min(0, self.W*state + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_policy:\n",
    "        \n",
    "    def __init__(self, sigma_w, sigma_b, params=None):\n",
    "        \n",
    "        # Store sigmas of the gaussian distributions\n",
    "        self.sigma_W = sigma_W\n",
    "        self.sigma_b = sigma_b\n",
    "        \n",
    "        # If params are given, initialize hyperpolicy means with them, else take random ones\n",
    "        if params == None:\n",
    "            params = np.random.rand(8)\n",
    "            \n",
    "        self.A_w   = params[0]\n",
    "        self.B_w   = params[1]\n",
    "        self.phi_w = params[2]\n",
    "        self.psi_w = params[3]\n",
    "        \n",
    "        self.A_b   = params[4]\n",
    "        self.B_b   = params[5]\n",
    "        self.phi_b = params[6]\n",
    "        self.psi_b = params[7]\n",
    "\n",
    "        \n",
    "    def params(self):\n",
    "        return [self.A_w, self.B_w, self.phi_w, self.psi_w,\n",
    "                self.A_b, self.B_b, self.phi_b, self.psi_b]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def W_mean(self, t):\n",
    "        return self.A_w * np.sin(self.phi_w*t + self.psi_w) + self.B_w\n",
    "    \n",
    "    def b_mean(self, t):\n",
    "        return self.A_b * np.sin(self.phi_b*t + self.psi_b) + self.B_b\n",
    "    \n",
    "    def W_pdf(self, W, t):\n",
    "        return scipy.stats.norm.pdf(W, loc=self.W_mean(t), scale=self.sigma_W)\n",
    "    \n",
    "    def b_pdf(self, b, t):\n",
    "        return scipy.stats.norm.pdf(b, loc=self.b_mean(t), scale=self.sigma_b)\n",
    "    \n",
    "    def theta_pdf(self, theta, t):\n",
    "        return W_pdf(theta[0],t) * b_pdf(theta[1],t)\n",
    "    \n",
    "    def sample_theta(self, t):\n",
    "        W = scipy.stats.norm.rvs(loc=self.W_mean(t), scale=self.sigma_W)\n",
    "        b = scipy.stats.norm.rvs(loc=self.b_mean(t), scale=self.sigma_b)\n",
    "        return W, b\n",
    "\n",
    "    def sample_policy(self, t):\n",
    "        W, b = self.sample_theta(t)\n",
    "        return linear_policy(W,b)\n",
    "    \n",
    "    \n",
    "    #def update_params(self, delta_params):\n",
    "        \n",
    "        # TO DO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Tools\n",
    "Here we define three possible performance estimators of our hyperpolicy $\\nu_{\\rho'}$ on the last $\\alpha$ samples collected by a behavioural hyperpolicy $\\nu_\\rho$. They are defined in the `performance_estimators.py` file and are:\n",
    "\n",
    " - Trajectory return estimation (IS weight based on entire trajectory):\n",
    "   $$ J^1_t(\\nu_\\rho) = \\left( \\prod_{i=t-\\alpha}^{t}  \\frac{\\nu_\\rho(\\theta_i\\vert t+1)}{\\nu_\\rho(\\theta_i\\vert i)} \\right) \\left(\\sum_{i=t-\\alpha}^{t} \\beta^{t-i} \\gamma^{i-t+\\alpha} R(s_i,\\pi_{\\theta_i}(s_i))\\right) $$\n",
    " - Per-step reward estimation (IS weight based on trajectory up to time $i$):\n",
    "   $$ J^2_t(\\nu_\\rho) =  \\sum_{i=t-\\alpha}^{t} \\beta^{t-i} \\gamma^{i-t+\\alpha} R(s_i,\\pi_{\\theta_i}(s_i))\\left( \\prod_{k=t-\\alpha}^{i} \\frac{\\nu_\\rho(\\theta_k\\vert t+1)}{\\nu_\\rho(\\theta_k\\vert k)} \\right) $$\n",
    " - Per-step reward estimation (IS weight based only on $i$-th timestep):\n",
    "   $$ J^3_t(\\nu_\\rho) =  \\sum_{i=t-\\alpha}^{t} \\beta^{t-i} \\gamma^{i-t+\\alpha} R(s_i,\\pi_{\\theta_i}(s_i)) \\frac{\\nu_\\rho(\\theta_i\\vert t+1)}{\\nu_\\rho(\\theta_i\\vert i)} $$\n",
    "A reliable estimated performance should be obtained by sampling $N$ different trajectories from the $\\nu_\\rho$ starting always from the state $s_{t-\\alpha}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play a random hyperpolicy, measure its performance and check if everything works correctly\n",
    "\n",
    "    # TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
