{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optimize_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d6c46a6e43f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptimize_pytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_then_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintertwine_run_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpareto_frontier_var_J1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optimize_pytorch'"
     ]
    }
   ],
   "source": [
    "# autoreload allows to update modules without resetting the kernel \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from optimize_pytorch import run_then_optimize, intertwine_run_optimize, pareto_frontier_var_J1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual bandits\n",
    "\n",
    "## Fixed t \n",
    "#### RBF Hyperpolicy on Periodic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    'env_class': 'cbenv.PeriodicBandit',     'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0.5,               'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    'A_c': 1,      'A_r': 1,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    'psi_c': 0,    'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    'phi_c': 0.1,  'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    'B_c': 0,       'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 15, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': True,\n",
    "    \n",
    "    # Run options\n",
    "    'alpha': 150,\n",
    "    'beta': 1,\n",
    "    'n_steps_sample': 150,\n",
    "    'epochs_optim': 2000,\n",
    "    'seeds': 123,\n",
    "    'save_folder': \"results/optimization/fixed_t/RBF_periodic_opt(1,0,1000)_lr1e-2_3mean-grad\",#_5seeds\",\n",
    "    #'save_folder': \"results/optimization/fixed_t/pytorch_RBF_test\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'optimizer_class': 'optim.RMSprop',\n",
    "    'learning_rate': 1e-2,\n",
    "    'grad_replicas': 3,\n",
    "    'lamb_J_1': 0,\n",
    "    'lamb_J_2': 1,\n",
    "    'lamb_v': 0,\n",
    "    'mean_reward': True,\n",
    "    #'fix_phi': False,\n",
    "    'use_modulo': False\n",
    "}\n",
    "\n",
    "run_then_optimize( **all_args )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Hyperpolicy on Vasicek environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    'env_class': 'cbenv.VasicekBandit',    'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0.5,  'sigma_r': 0.5,   'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    'A_c': 0.05,        'A_r': 0.05,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    #'psi_c': 0,      'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    #'phi_c': 0.1,    'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    'B_c': 0,        'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 15, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': True,\n",
    "    \n",
    "    # Run options\n",
    "    'alpha': 150,\n",
    "    'n_steps_sample': 150,\n",
    "    'epochs_optim': 500,\n",
    "    #'seeds': 20,\n",
    "    'save_folder': \"results/optimization/fixed_t/RBF_vasicek_opt(05,1,0)_infer-stds_3mean-grad_1000epochs\",\n",
    "    'save_folder': \"results/optimization/fixed_t/RBF_vasicek_opt(0,1,0)\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'learning_rate': 1e-2,\n",
    "    'grad_replicas': 3, \n",
    "    'lamb_J_1': 0,\n",
    "    'lamb_J_2': 1,\n",
    "    'lamb_v': 0,\n",
    "    'mean_reward': True,\n",
    "    #'fix_phi': False,\n",
    "    'use_modulo': False\n",
    "}\n",
    "\n",
    "run_then_optimize( **all_args )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Hyperpolicy on \"Unemployment Rate\" (Unrate) environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_then_optimize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf352acd2c4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mrun_then_optimize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_args\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'run_then_optimize' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    'env_class': 'cbenv.UnrateBandit',     'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0.5,                   'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    #'A_c': 1,      'A_r': 1,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    #'psi_c': 0,    'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    #'phi_c': 0.1,  'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    #'B_c': 0,       'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 15, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': True,\n",
    "    \n",
    "    # Run options\n",
    "    'alpha': 150,\n",
    "    'beta': 1,\n",
    "    'n_steps_sample': 150,\n",
    "    'epochs_optim': 1000,\n",
    "    #'seeds': 123,\n",
    "    'save_folder': \"results/optimization/fixed_t/RBF_unrate_dyn_opt(1,0,0)_lr1e-2_3mean-grad\",#_5seeds\",\n",
    "    #'save_folder': \"results/optimization/fixed_t/pytorch_RBF_test\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'optimizer_class': 'optim.RMSprop',\n",
    "    'learning_rate': 1e-2,\n",
    "    'grad_replicas': 3,\n",
    "    'lamb_J_1': 1,\n",
    "    'lamb_J_2': 0,\n",
    "    'lamb_v': 0,\n",
    "    'mean_reward': True,\n",
    "    #'fix_phi': False,\n",
    "    'use_modulo': False\n",
    "}\n",
    "\n",
    "\n",
    "run_then_optimize( **all_args )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Hyperpolicy on dynamics of \"Unemployment Rate\" (Unrate) environment\n",
    "Learn only if future context will be greater or lower than current one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    'env_class': 'cbenv.UnrateBandit_dynamics',     'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0,                   'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    #'A_c': 1,      'A_r': 1,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    #'psi_c': 0,    'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    #'phi_c': 0.1,  'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    #'B_c': 0,       'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 15, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': True,\n",
    "    \n",
    "    # Run options\n",
    "    'alpha': 150,\n",
    "    'beta': 1,\n",
    "    'n_steps_sample': 150,\n",
    "    'epochs_optim': 1000,\n",
    "    #'seeds': 123,\n",
    "    'save_folder': \"results/optimization/fixed_t/RBF_unrate_dyn_opt(1,0,0)_lr1e-2_3mean-grad\",#_5seeds\",\n",
    "    #'save_folder': \"results/optimization/fixed_t/pytorch_RBF_test\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'optimizer_class': 'optim.RMSprop',\n",
    "    'learning_rate': 1e-2,\n",
    "    'grad_replicas': 3,\n",
    "    'lamb_J_1': 1,\n",
    "    'lamb_J_2': 0,\n",
    "    'lamb_v': 0,\n",
    "    'mean_reward': True,\n",
    "    #'fix_phi': False,\n",
    "    'use_modulo': False\n",
    "}\n",
    "\n",
    "\n",
    "run_then_optimize( **all_args )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intertwined optimization\n",
    "#### Periodic hyperpolicy on Periodic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Define all arguments for 'intertwine_run_optimize()'   PERIODIC ENV and PERIODIC HYPERPOLICY\n",
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    #'sigma_c': 0.5,               'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    #'sigma_x': 0.5,  'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,    'sigma_r': 0.1, \n",
    "    #'A_x': 3,      'A_theta_behavioural': 3,       'A_theta_init': 3,        'A_r': 3, \n",
    "    #'psi_x': 0,    'psi_theta_behavioural': 0,     'psi_theta_init': 0.5,      'psi_r': 0, \n",
    "    #'phi_x': 0.1,    'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.1,   'phi_r': 0.1, \n",
    "    #'B_x': 0,      'B_theta_behavioural': 0,       'B_theta_init': 0.2,        'B_r': 0, \n",
    "\n",
    "    'env_class': 'cbenv.PeriodicBandit',    'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0.5,               'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    'A_c': 1,      'A_r': 1,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    'psi_c': 0,    'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    'phi_c': 0.1,  'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    'B_c': 0,       'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 15, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': False,\n",
    "    \n",
    "    # Run options\n",
    "    'alpha': 150,\n",
    "    'n_init_samples': 150,\n",
    "    'n_optim_samples': 50,\n",
    "    'optim_every': 1,\n",
    "    'epochs_optim': 100,\n",
    "    'seeds': 123456,  # DOES NOT WORK?\n",
    "    #'save_folder': \"results/optimization/intertwine/pytorch_RBF_optJ1_lr1e-2_3mean-grad_infer-cent_boostedR\",\n",
    "    'save_folder': \"results/optimization/intertwine/pytorch_RBF_test_\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'grad_replicas': 3,\n",
    "    'learning_rate': 1e-2,\n",
    "    'lamb_J_1': 1,\n",
    "    'lamb_J_2': 0,\n",
    "    'lamb_v': 0,\n",
    "    'mean_reward': True,\n",
    "    #'fix_phi': False,\n",
    "    'use_modulo': False\n",
    "}\n",
    "\n",
    "### Run optimization with previous args\n",
    "start = time()\n",
    "intertwine_run_optimize( **all_args )\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Hyperpolicy on Periodic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### Define all arguments for 'intertwine_run_optimize()'   PERIODIC ENV and RBF\n",
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    'env_class': 'cbenv.PeriodicBandit',    'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0.5,               'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    'A_c': 1,      'A_r': 1,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    'psi_c': 0,    'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    'phi_c': 0.1,  'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    'B_c': 0,       'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 15, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': False,\n",
    "    \n",
    "    # Run options\n",
    "    'alpha': 150,\n",
    "    'n_init_samples': 150,\n",
    "    'n_optim_samples': 200,\n",
    "    'optim_every': 1,\n",
    "    'epochs_optim': 1000,\n",
    "    #'seeds': 123456,  # DOES NOT WORK?\n",
    "    'save_folder': \"results/optimization/intertwine/RBF_periodic_optJ1_lr1e-2_3mean-grad_boostedR_200steps_100-optims\",\n",
    "    #'save_folder': \"results/optimization/intertwine/pytorch_RBF_test_\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'grad_replicas': 3,\n",
    "    'learning_rate': 1e-2,\n",
    "    'lamb_J_1': 1,\n",
    "    'lamb_J_2': 0,\n",
    "    'lamb_v': 1,\n",
    "    'mean_reward': True,\n",
    "    #'fix_phi': False,\n",
    "    'use_modulo': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Hyperpolicy on Vasicek environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    'env_class': 'cbenv.VasicekBandit',    'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0.5,  'sigma_r': 1,   'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    'A_c': 0.05,        'A_r': 0.05,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    #'psi_c': 0,      'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    #'phi_c': 0.1,    'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    'B_c': 0,        'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 15, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': False,\n",
    "    #            15               6\n",
    "    \n",
    "    \n",
    "    # Run options\n",
    "    'alpha': 150,\n",
    "    'n_init_samples': 150,\n",
    "    'n_optim_samples': 200,\n",
    "    'optim_every': 1,\n",
    "    'epochs_optim': 50,\n",
    "    'seeds': 12345,\n",
    "    'save_folder': \"results/optimization/intertwine/RBF_Vasicek_optJ1_boostedR_3mean-grad_25rbfs_200steps_50-optims\",\n",
    "    #'save_folder': \"results/optimization/intertwine/pytorch_RBF_test_\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'grad_replicas': 3,\n",
    "    'learning_rate': 1e-2,\n",
    "    'lamb_J_1': 1,\n",
    "    'lamb_J_2': 0,\n",
    "    'lamb_v': 0,\n",
    "    'mean_reward': True,\n",
    "    #'fix_phi': False,\n",
    "    'use_modulo': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Frontier optimization\n",
    "#### J1-Var frontier (for RBF and periodic env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define all arguments for 'pareto_frontier_var_J1()' with PERIODIC env\n",
    "all_args = {  \n",
    "    \n",
    "    # Parameters for each non-stationary process\n",
    "    'env_class': 'cbenv.PeriodicBandit',     'hyperpolicy_class': 'hypol.RBF_Policy',\n",
    "    'sigma_c': 0.5,               'sigma_theta_behavioural': 0.1,   'sigma_theta_init': 0.1,\n",
    "    'A_c': 1,      'A_r': 1,      #'A_theta_behavioural': 1,       'A_theta_init': 1,        \n",
    "    'psi_c': 0,    'psi_r': 0,    #'psi_theta_behavioural': 0,     'psi_theta_init': 0,      \n",
    "    'phi_c': 0.1,  'phi_r': 0.1,  #'phi_theta_behavioural': 0.1,   'phi_theta_init': 0.09,   \n",
    "    'B_c': 0,       'B_r': 0,     #'B_theta_behavioural': 0,       'B_theta_init': 0,       \n",
    "    'rbf_nodes': 17, 'sigma_rbf': 6, 'infer_centers': True, 'infer_stds': True,\n",
    "    \n",
    "    # Run options\n",
    "    'beta': 20,\n",
    "    'two_step_min_psi': True,\n",
    "    'alpha': 150,\n",
    "    'n_steps_sample': 150,\n",
    "    'epochs_optim': 600,\n",
    "    'seeds': [123,456,789],\n",
    "    #'seeds': [1,2,3],\n",
    "    'save_folder': 'results/optimization/pareto/l1-(1,1)_lv-(1,5000)_RBF_periodic_3-mean-grad_3-seeds_beta-10',\n",
    "    #'save_folder': \"results/optimization/pareto/pytorch_RBF_test\",\n",
    "    \n",
    "    # Optimization settings\n",
    "    'optimizer_class': 'optim.RMSprop',\n",
    "    'learning_rate': 1e-2,\n",
    "    'grad_replicas': 3,\n",
    "    'lambda_J1_values': np.ones(15),\n",
    "    'lamb_J_2': 0,\n",
    "    'lambda_v_values': np.linspace(1,5000,15),\n",
    "    'average_frontier': True,\n",
    "    'mean_reward': True,\n",
    "    'use_modulo': False\n",
    "}\n",
    "\n",
    "pareto_frontier_var_J1( **all_args )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
