{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Stationary Contextual Bandits Evaluation\n",
    "\n",
    "In this notebook we will build a simple setting (contextual bandits) to understand how a non-stationary policy should be evaluated. \n",
    "\n",
    "#### Contextual bandits\n",
    "We introduce a contextual bandit with 2 arms, where the choice of its actions $a_t\\in\\{0,1\\}$ also depends on its context $x_t$, which is sampled from a Context distribution $\\mathcal{D}$. Since we want a non-stationary contextual bandit, this distribution will change smoothly over time, so the context is sampled from a time-dependend distribution $x_t\\sim\\mathcal{D}_t$.\n",
    "Moreover, at each time $t$ the policy parameter will be given by a time-dependent hyperpolicy $\\nu$ that is tuned with the hyperparameters $\\rho$.\n",
    "\n",
    "#### Problem definition \n",
    "Define the non-stationary process that builds the context distributions $\\mathcal{D}_t$ as a sinusoidal one, i.e. we have $\\mathcal{D}_t = \\mathcal{N}(\\mu_x(t),\\sigma_x)$ with $\\mu_x(t) = A_x\\sin(\\phi_x t + \\psi_x) + B_x$ and a constant $\\sigma_x$. We want the agent to recognise if $x_t>\\mu_x(t)$ or if $x_t<\\mu_x(t)$. The two possible actions represent those two cases, and the reward is equal to $|x_t-\\mu_x(t)|$ if the agent is right and $-|x_t-\\mu_x(t)|$ if it is wrong. In order to handle the non-stationary process, the hyperpolicy needs to be based on a similar sinusoidal process, and its goal should be learning and replicating the non-stationary process in order to maximize the rewards.\n",
    "\n",
    "In the end, the bandit can be represented in two ways, depending on the desired approach:\n",
    " - **Action-based exploration**: The stochasticity is given by the policy, which will be represented by a Bernoulli distribution $\\pi_{\\theta_t}(a_t|x_t) = \\left(p_t, 1-p_t\\right)$, where $p_t \\equiv f(\\theta_t, x_t) = S(x_t-\\theta_t)$ (with $S(x)$ the sigmoid function) and $\\theta_t \\sim \\nu_\\rho(\\theta_t|t)$. In this case, the hyperpolicy is deterministic and, at each $t$, the value of $\\theta_t$ is given by $\\theta_t = \\nu_\\rho(t) = A_\\theta\\sin(\\phi_\\theta t + \\psi_\\theta) + B_\\theta$.\n",
    " - **Parameter-based exploration**: The stochasticity is given by the hyperpolicy, which is represented by a time-dependent gaussian distribution $\\nu_\\rho(\\theta_t|t) = \\mathcal{N}(\\mu_\\theta(t),\\sigma_\\theta)$ with $\\mu_\\theta(t) = A_\\theta\\sin(\\phi_\\theta t + \\psi_\\theta) + B_\\theta$ and a constant $\\sigma_\\theta$. The sampled $\\theta_t$ will now define a deterministic policy $\\pi_{\\theta_t}(x_t)$, which will be represented by a step function centered in $(x_t-\\theta_t)$ and will be equal to $1$ if $x_t>\\theta_t$ and $0$ if $x_t<\\theta_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environment\n",
    "\n",
    "Start by defining the environment that contains the non-stationary context and that assigns rewards for each action of the bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class environment:\n",
    "    \n",
    "    def __init__(self, t0=0, sigma_x=1, **kwargs):\n",
    "        \n",
    "        # Store the given parameters\n",
    "        self.t = t0\n",
    "        self.sigma_x = sigma_x\n",
    "        \n",
    "        # if params that define the mean are not given, take random ones\n",
    "        var_names = ['A_x', 'B_x', 'phi_x', 'psi_x'] \n",
    "        for var in var_names:\n",
    "            if var in kwargs.keys():\n",
    "                setattr(self, var, kwargs[var])\n",
    "            else:\n",
    "                setattr(self, var, np.random.rand())\n",
    "    \n",
    "    \n",
    "    # Increase the time variable of 'delta_t' steps\n",
    "    def increase_t(self, delta_t=1):\n",
    "        self.t += delta_t\n",
    "    \n",
    "    \n",
    "    # Compute the context mean in a given istant (or in 'self.t')\n",
    "    def x_mean(self, t=None):\n",
    "        if t == None:\n",
    "            return self.A_x * np.sin(self.phi_x*self.t + self.psi_x) + self.B_x\n",
    "        else:\n",
    "            return self.A_x * np.sin(self.phi_x*t + self.psi_x) + self.B_x\n",
    "    \n",
    "    # Sample a context from the distibution at time 'self.t'\n",
    "    def sample_x(self):\n",
    "        return scipy.stats.norm.rvs(loc   = self.x_mean(), \n",
    "                                    scale = self.sigma_x)\n",
    "    \n",
    "    # Obtain reward of the action 'over_mu': positive if correct\n",
    "    #                                        negative if wrong\n",
    "    def get_reward(self, x_t, over_mu):\n",
    "        if over_mu == (x_t > self.x_mean()):\n",
    "            return np.abs(x_t-self.x_mean())\n",
    "        else: \n",
    "            return -np.abs(x_t-self.x_mean())\n",
    "    \n",
    "    \n",
    "    #def play(self, nu, n_steps):\n",
    "        \n",
    "        # TO DO\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-policy and policy\n",
    "\n",
    "Here we will define the classes for the hyperpolicies and policies. We will follow the structure described at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy:\n",
    "    \n",
    "    def __init__(self, theta, stochastic=False):\n",
    "        self.theta = theta\n",
    "        self.stochastic = stochastic\n",
    "        \n",
    "    def params(self):\n",
    "        return self.theta\n",
    "        \n",
    "    def action(self, x):\n",
    "        if self.stochastic:\n",
    "            p = 1 / (1 + np.exp(-(x-self.theta)))   ### CHECK IF IT IS CORRECT\n",
    "            return scipy.stats.bernoulli.rvs(p)\n",
    "        else:\n",
    "            return (x > theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperpolicy:\n",
    "        \n",
    "    def __init__(self, sigma_theta=1, **kwargs):\n",
    "        \n",
    "        # Store sigmas of the gaussian distributions\n",
    "        self.sigma_W = sigma_W\n",
    "        self.sigma_b = sigma_b\n",
    "        \n",
    "        # List of other params that can be set\n",
    "        self.var_names = ['A_W', 'B_W', 'phi_W', 'psi_W'] \n",
    "        \n",
    "        # If params are given, initialize hyperpolicy means with them, else take random ones\n",
    "        for var in self.var_names:\n",
    "            if var in kwargs.keys():\n",
    "                setattr(self, var, kwargs[var])\n",
    "            else:\n",
    "                setattr(self, var, np.random.rand())\n",
    "               \n",
    "            \n",
    "        \n",
    "    def params(self):\n",
    "        values = [self.A_W, self.B_W, self.phi_W, self.psi_W,\n",
    "                  self.A_b, self.B_b, self.phi_b, self.psi_b]\n",
    "        return {x:y for x,y in zip(self.var_names, values)}\n",
    "    \n",
    "    \n",
    "    def W_mean(self, t):\n",
    "        return self.A_W * np.sin(self.phi_W*t + self.psi_W) + self.B_W\n",
    "    \n",
    "    def b_mean(self, t):\n",
    "        return self.A_b * np.sin(self.phi_b*t + self.psi_b) + self.B_b\n",
    "    \n",
    "    def W_pdf(self, W, t):\n",
    "        return scipy.stats.norm.pdf(W, loc=self.W_mean(t), scale=self.sigma_W)\n",
    "    \n",
    "    def b_pdf(self, b, t):\n",
    "        return scipy.stats.norm.pdf(b, loc=self.b_mean(t), scale=self.sigma_b)\n",
    "    \n",
    "    def theta_pdf(self, theta, t):\n",
    "        return self.W_pdf(theta[0],t) * self.b_pdf(theta[1],t)\n",
    "    \n",
    "    \n",
    "    def sample_theta(self, t):\n",
    "        W = scipy.stats.norm.rvs(loc=self.W_mean(t), scale=self.sigma_W)\n",
    "        b = scipy.stats.norm.rvs(loc=self.b_mean(t), scale=self.sigma_b)\n",
    "        return W, b\n",
    "\n",
    "    def sample_policy(self, t):\n",
    "        W, b = self.sample_theta(t)\n",
    "        return linear_policy(W,b)\n",
    "    \n",
    "    \n",
    "    #def update_params(self, delta_params):\n",
    "        \n",
    "        # TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
