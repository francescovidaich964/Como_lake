{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Stationary Contextual Bandits Evaluation\n",
    "\n",
    "In this notebook we will build a simple setting (contextual bandits) to understand how a non-stationary policy should be evaluated. \n",
    "\n",
    "#### Contextual bandits\n",
    "We introduce a contextual bandit with 2 arms, where the choice of its actions $a_t\\in\\{0,1\\}$ also depends on its context $x_t$, which is sampled from a Context distribution $\\mathcal{D}$. Since we want a non-stationary contextual bandit, this distribution will change smoothly over time, so the context is sampled from a time-dependend distribution $x_t\\sim\\mathcal{D}_t$.\n",
    "Moreover, at each time $t$ the policy parameter will be given by a time-dependent hyperpolicy $\\nu$ that is tuned with the hyperparameters $\\rho$.\n",
    "\n",
    "#### Problem definition \n",
    "Define the non-stationary process that builds the context distributions $\\mathcal{D}_t$ as a sinusoidal one, i.e. we have $\\mathcal{D}_t = \\mathcal{N}(\\mu_x(t),\\sigma_x)$ with $\\mu_x(t) = A_x\\sin(\\phi_x t + \\psi_x) + B_x$ and a constant $\\sigma_x$. We want the agent to recognise if $x_t>\\mu_x(t)$ or if $x_t<\\mu_x(t)$. The two possible actions represent those two cases, and the reward is equal to $|x_t-\\mu_x(t)|$ if the agent is right and $-|x_t-\\mu_x(t)|$ if it is wrong. In order to handle the non-stationary process, the hyperpolicy needs to be based on a similar sinusoidal process, and its goal should be learning and replicating the non-stationary process in order to maximize the rewards.\n",
    "\n",
    "In the end, the bandit can be represented in two ways, depending on the desired approach:\n",
    " - **Action-based exploration**: The stochasticity is given by the policy, which will be represented by a Bernoulli distribution $\\pi_{\\theta_t}(a_t|x_t) = \\left(p_t, 1-p_t\\right)$, where $p_t \\equiv f(\\theta_t, x_t) = S(x_t-\\theta_t)$ (with $S(x)$ the sigmoid function) and $\\theta_t \\sim \\nu_\\rho(\\theta_t|t)$. In this case, the hyperpolicy is deterministic and, at each $t$, the value of $\\theta_t$ is given by $\\theta_t = \\nu_\\rho(t) = A_\\theta\\sin(\\phi_\\theta t + \\psi_\\theta) + B_\\theta$.\n",
    " - **Parameter-based exploration**: The stochasticity is given by the hyperpolicy, which is represented by a time-dependent gaussian distribution $\\nu_\\rho(\\theta_t|t) = \\mathcal{N}(\\mu_\\theta(t),\\sigma_\\theta)$ with $\\mu_\\theta(t) = A_\\theta\\sin(\\phi_\\theta t + \\psi_\\theta) + B_\\theta$ and a constant $\\sigma_\\theta$. The sampled $\\theta_t$ will now define a deterministic policy $\\pi_{\\theta_t}(x_t)$, which will be represented by a step function centered in $(x_t-\\theta_t)$ and will be equal to $1$ if $x_t>\\theta_t$ and $0$ if $x_t<\\theta_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment definition\n",
    "\n",
    "Start by defining the environment that contains the non-stationary context and that assigns rewards for each action of the bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class environment:\n",
    "    \n",
    "    def __init__(self, t0=0, sigma_x=1, **kwargs):\n",
    "        \n",
    "        # Store the given parameters\n",
    "        self.t = t0\n",
    "        self.sigma_x = sigma_x\n",
    "        \n",
    "        # if params that define the mean are not given, take random ones\n",
    "        self.var_names = ['A_x', 'B_x', 'phi_x', 'psi_x'] \n",
    "        for var in self.var_names:\n",
    "            if var in kwargs.keys():\n",
    "                setattr(self, var, kwargs[var])\n",
    "            else:\n",
    "                setattr(self, var, np.random.rand())\n",
    "    \n",
    "    \n",
    "    # Return params of the non-stat environment\n",
    "    def params(self):\n",
    "        values = [self.A_W, self.B_W, self.phi_W, self.psi_W]\n",
    "        return {x:y for x,y in zip(self.var_names, values)}\n",
    "   \n",
    "    \n",
    "    # Compute the context mean in a given istant (or in 'self.t')\n",
    "    def x_mean(self, t=None):\n",
    "        if t == None:\n",
    "            return self.A_x * np.sin(self.phi_x*self.t + self.psi_x) + self.B_x\n",
    "        else:\n",
    "            return self.A_x * np.sin(self.phi_x*t + self.psi_x) + self.B_x\n",
    "    \n",
    "    # Sample a context from the distibution at time 'self.t'\n",
    "    def sample_x(self):\n",
    "        return scipy.stats.norm.rvs(loc   = self.x_mean(), \n",
    "                                    scale = self.sigma_x)\n",
    "    \n",
    "    \n",
    "    # Obtain reward of the action 'over_mu': positive if correct\n",
    "    #                                        negative if wrong\n",
    "    def get_reward(self, x_t, over_mu):\n",
    "        if over_mu == (x_t > self.x_mean()):\n",
    "            return np.abs(x_t-self.x_mean())\n",
    "        else: \n",
    "            return -np.abs(x_t-self.x_mean())\n",
    "    \n",
    "    \n",
    "    # Use the hyperpolicy nu to perform many subsequent steps\n",
    "    # Returns the evolution of many variables\n",
    "    def play(self, nu, n_steps, optimization=False):\n",
    "        \n",
    "        contexts = np.array([])\n",
    "        rewards  = np.array([])\n",
    "        context_means = np.array([])\n",
    "        theta_means   = np.array([])\n",
    "    \n",
    "        # At each timestep..\n",
    "        for i in range(n_steps):\n",
    "            \n",
    "            # Sample a policy from the hyperpolicy\n",
    "            policy = nu.sample_policy(self.t)\n",
    "            \n",
    "            # Sample a context from the corresponding distribution\n",
    "            x_t = self.sample_x()\n",
    "            \n",
    "            # Perform a step sampling the action from current policy \n",
    "            action = policy.action(x_t)\n",
    "            reward = self.get_reward(action)\n",
    "            \n",
    "            # Optimize hyperpolicy if flag is 'True'\n",
    "            if optimize:\n",
    "                \n",
    "                # DEFINE OPTIMIZATION PROCEDURE\n",
    "                print('TO DO')\n",
    "                \n",
    "            \n",
    "            # Store current values in the arrays\n",
    "            contexts = np.append(contexts, x_t)\n",
    "            rewards = np.append(rewards, reward)\n",
    "            context_means = np.append(context_means, self.x_mean())\n",
    "            theta_means = np.append(theta_means, nu.theta_mean(self.t))\n",
    "\n",
    "        return contexts, rewards, context_means, theta_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy definition\n",
    "\n",
    "Here we will define the policy class following the structure described at the beginning of the notebook. The action is given by a bool variable (`True` if policy thinks that $x_t>\\mu_x(t)$, `False` otherwise). \n",
    "\n",
    "If `stochastic = True`, the policy sample the action from the Bernoulli distribution centered in $x_t-\\theta_t$, otherwise the policy is just a step function centered in the same quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy:\n",
    "    \n",
    "    def __init__(self, theta, stochastic=True):\n",
    "        self.theta = theta\n",
    "        self.stochastic = stochastic\n",
    "        \n",
    "    def params(self):\n",
    "        return self.theta\n",
    "        \n",
    "    def action(self, x):\n",
    "        if self.stochastic:\n",
    "            p = 1 / (1 + np.exp(-(x-self.theta)))   ### CHECK IF IT IS CORRECT (it should be)\n",
    "            return scipy.stats.bernoulli.rvs(p)\n",
    "        else:\n",
    "            return (x > theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperpolicy definition\n",
    "\n",
    "Here we will define the hyperpolicy class following the structure described at the beginning of the notebook. If `stochastic = True`, the hyperpolicy samples the $\\theta_t$ value from the gaussian distribution with a time dependent mean $\\mu(t)$, otherwise we just have $\\theta_t = \\mu(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperpolicy:\n",
    "        \n",
    "    def __init__(self, sigma_theta=1, stochastic=True, **kwargs):\n",
    "             \n",
    "        # Store the given parameters\n",
    "        self.sigma_theta = sigma_theta\n",
    "        self.stochastic = stochastic\n",
    "        \n",
    "        # if params that define the mean are not given, take random ones        \n",
    "        self.var_names = ['A_theta', 'B_theta', 'phi_theta', 'psi_theta'] \n",
    "        for var in self.var_names:\n",
    "            if var in kwargs.keys():\n",
    "                setattr(self, var, kwargs[var])\n",
    "            else:\n",
    "                setattr(self, var, np.random.rand())\n",
    "               \n",
    "            \n",
    "        \n",
    "    def params(self):\n",
    "        values = [self.A_theta, self.B_theta, self.phi_theta, self.psi_theta]\n",
    "        return {x:y for x,y in zip(self.var_names, values)}\n",
    "    \n",
    "    \n",
    "    def theta_mean(self, t):\n",
    "        return self.A_W * np.sin(self.phi_W*t + self.psi_W) + self.B_W\n",
    "        \n",
    "    def theta_pdf(self, theta):\n",
    "        return scipy.stats.norm.pdf(theta, loc=self.theta_mean(t), scale=self.sigma_theta)\n",
    "    \n",
    "    def sample_theta(self, t):\n",
    "        if self.stochastic:\n",
    "            return scipy.stats.norm.rvs(loc=self.theta_mean(t), scale=self.sigma_theta)\n",
    "        else:\n",
    "            return self.theta_mean(t)\n",
    "        \n",
    "\n",
    "    def sample_policy(self, t):\n",
    "        theta = self.sample_theta(t)\n",
    "        return policy(theta, not(self.stochastic) )\n",
    "    \n",
    "    \n",
    "    #def update_params(self, delta_params):\n",
    "        \n",
    "        # TO DO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test environment with random hyperpolicy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
