{'n_steps_sample': 200, 'epochs_optim': 500, 'hyperpolicy_class': <class 'policy.hyperpolicy_parallel.HyperPolicy'>, 'policy_class': <class 'policy.policy.Policy'>, 'env_class': <class 'env.contextual_bandits_parallel.DriftBandit'>, 'seed': None, 'save_folder': 'results/optimization/parallel/drift_test_many_A', 'optimizer_class': <class 'optimization.optimizers_parallel.RMSProp'>, 'alpha': 200, 'learning_rate': 0.0002, 'lamb_J_1': 1, 'lamb_J_2': 0, 'lamb_v': 0, 'mean_reward': True, 'sigma_c': 0.5, 'sigma_theta_behavioural': 0.1, 'sigma_theta_init': 0.1, 'sigma_r': 1, 'A_c': 0.01, 'A_theta_behavioural': 0.01, 'A_theta_init': [0, 0.5, 1, 1.5, 2], 'A_r': 0.01, 'psi_c': 0, 'psi_theta_behavioural': 0, 'psi_theta_init': 0, 'psi_r': 0, 'phi_c': 1, 'phi_theta_behavioural': 1, 'phi_theta_init': 1, 'phi_r': 1, 'B_c': 0, 'B_theta_behavioural': 0, 'B_theta_init': 0, 'B_r': 0, 'use_modulo': False, 'R_inf': 1, 'grad_replicas': 3, 'beta': 1, 'n_seeds': 1}