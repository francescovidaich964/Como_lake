{'sigma_c': 0.5, 'sigma_r': 0, 'sigma_theta_behavioural': 0.1, 'sigma_theta_init': 0.1, 'A_c': 0.01, 'A_r': 0.01, 'A_theta_behavioural': 0.01, 'A_theta_init': [0, 0.5, 1, 1.5, 2], 'psi_c': 0, 'psi_r': 0, 'psi_theta_behavioural': 0, 'psi_theta_init': [-1, -0.5, 0, 0.5, 1], 'phi_c': 1, 'phi_r': 0.05, 'phi_theta_behavioural': 0.05, 'phi_theta_init': 0.1, 'B_c': 0, 'B_r': 0, 'B_theta_behavioural': 0, 'B_theta_init': 0, 'hyperpolicy_class': <class 'policy.hyperpolicy_parallel.HyperPolicy'>, 'policy_class': <class 'policy.policy.Policy'>, 'env_class': <class 'env.contextual_bandits_parallel.PeriodicBandit'>, 'epochs_optim': 500, 'optimizer_class': <class 'optimization.optimizers_parallel.RMSProp'>, 'learning_rate': 0.0002, 'lamb_J_1': 1, 'lamb_J_2': 0, 'lamb_v': 0, 'R_inf': 1, 'grad_replicas': 3, 'seed': None, 'n_steps_sample': 200, 'alpha': 200, 'beta': 1, 'save_folder': 'results/optimization/parallel/drift_test_many_A', 'mean_reward': True, 'use_modulo': False, 'n_seeds': 1}